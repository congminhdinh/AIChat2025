## CHAPTER 5: CÁC GIẢI PHÁP VÀ ĐÓNG GÓP NỔI BẬT (Solutions & Contributions) - Min 5 pages

**Critical**: This chapter determines your evaluation score. Show creativity, analysis, and problem-solving.

### 5.1 Hierarchical Data Modeling and Context-Aware Chunking

#### Problem
- **Loss of Legal Context**: In the Vietnamese legal system, a Decree (*Nghị định*) is meaningless without reference to the Law (*Luật*) it guides. Standard RAG retrieval treats all documents as isolated islands.
- **Ambiguity in Vector Search**: A chunk from a Decree might mention "Fine of 5,000,000 VND," but without knowing which Law it belongs to, the LLM cannot apply it correctly.
- **Flat Data Structures**: Naive database designs cannot efficiently represent the recursive "Law → Decree → Circular" relationship.

#### Solution
**1. Self-Referencing Entity Model (The "Chain of Validity"):**
- Implemented a self-referencing `Documents` table strategy.
- Logic: `Type 1` (Law) acts as the root. `Type 2` (Decree) contains a `FatherDocumentId` pointing to the Law.
- This creates a hard-linked "skeleton" of legal validity before any AI processing occurs.

**2. Metadata Enrichment at Ingestion:**
- **Dynamic Parent Lookup**: During the chunking process for a Decree, the system performs a reverse lookup to fetch the `DocumentName` of the parent Law.
- **Context Injection**: The parent document's name is injected into the Qdrant payload (`father_doc_name`) and the embedding context.
- **Structure**:
  ```csharp
  // Data pushed to Qdrant
  {
      "content": "Article 5: Penalties for violations...",
      "metadata": {
          "doc_type": "Decree",
          "father_doc_name": "Law on Cyber Security 2018", // Crucial context added
          "heading1": "Chapter II",
          "heading2": "Section 1"
      }
  }
  ```

### 5.2 Compliance-Driven Dual-RAG Architecture

#### Problem
- Users need both: "What does my company policy say?" AND "Is it legal?"
- Single knowledge base insufficient for compliance checking
- Post-hoc verification requires multiple LLM calls

#### Solution
**Parallel Dual-Source Retrieval:**
```
1. Generate query embedding
2. Execute parallel searches:
   - Search 1: Filter tenant_id = current_tenant (company policies)
   - Search 2: Filter tenant_id = 1 (national legal framework)
3. Retrieve top-3 from each source
4. Structure context with source labels:
   [Internal Company Regulations]
   ... company policy chunks ...
   [Legal Framework Documents]
   ... statutory law chunks ...
5. LLM reasoning template:
   - Answer based on company policy
   - Cross-reference with statutory requirements
   - Flag violations with warning
```

**Qdrant Filter Logic:**
```python
search_query = {
    "filter": {
        "should": [
            {"match": {"tenant_id": current_tenant_id}},
            {"match": {"tenant_id": 1}}  # Shared legal framework
        ]
    },
    "limit": 6
}
```

#### Results
- Single-pass compliance verification
- Users receive warnings when policies violate laws
- Reduces legal risk for organizations

### 5.3 Infrastructure-Level Tenant Context Propagation

#### Problem
- Microservices require tenant context at every layer
- Manual tenant passing error-prone
- Background jobs have no HTTP context

#### Solution
**JWT-Based Tenant Continuity:**
```
1. Account Service generates JWT with claims: user_id, tenant_id, username, is_admin
2. All services share identical JWT configuration
3. ICurrentUserProvider abstraction extracts tenant from HTTP context
4. Hybrid provider for background jobs: try HTTP context → fallback to thread-local storage
5. Message payloads carry tenant_id for impersonation
6. Vector queries automatically filter by extracted tenant_id
```

**Defense-in-Depth:**
- Layer 1: Gateway validates JWT
- Layer 2: Services independently validate JWT
- Layer 3: EF Core query filter adds tenant WHERE clause
- Layer 4: Database interceptor stamps tenant_id on inserts
- Layer 5: Vector DB metadata filter enforces isolation

#### Results
- Zero cross-tenant data leakage in testing
- Compile-time safety via dependency injection
- Seamless tenant context in sync and async operations

### 5.4 Asynchronous Distributed AI Processing Pipeline

#### Problem
- LLM inference takes 30+ seconds
- Long HTTP timeouts hold server resources
- Need scalable AI processing

#### Solution
**Event-Driven Architecture:**
```
User → Gateway → Chat Service → [Persist Message] → RabbitMQ → Chat Processor → [LLM Inference] → RabbitMQ → Chat Service → SignalR → User
```

**Key Design Decisions:**
- Chat Service returns 202 Accepted immediately
- RabbitMQ decouples request from processing
- Prefetch=1 prevents worker starvation
- SignalR provides real-time response delivery
- Hangfire for document processing background jobs

#### Results
- Horizontal scaling: Add more Chat Processor instances
- Fault tolerance: Message requeue on failure
- User experience: Non-blocking interface

---