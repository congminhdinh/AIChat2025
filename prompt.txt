Role: Senior Python AI Engineer.

Context: We are enhancing the ChatProcessor.py service in our project: "Multi-tenant RAG Application for building a website to look up and answer laws and customized internal regulations for each enterprise." To evaluate our RAG performance later using the Ragas framework, we need to log specific metadata for every generated response. 

The Strategy: Instead of running evaluation logic in real-time or using a message queue, we will log the interaction details into a local JSON file. This file will serve as an offline dataset for batch evaluation later. 

Task Requirements:

1. Dependency Management:

Add ragas to the requirements.txt file (for future use). 

2. Metadata Structure: Immediately after the LLM generates a response, construct a metadata dictionary containing the following fields:

question: The original user query.


contexts: A list of strings containing the retrieved document chunks (Internal Policy + Global Law). 

answer: The final response generated by the LLM.

conversation_id: Unique identifier for the chat session.

user_id: Unique identifier for the user.


tenant_id: The specific ID of the enterprise (Tenant). 

timestamp: The ISO 8601 formatted time of the event.

3. Logging Logic (File-based):

Implement a function to save this metadata into a file named evaluation_logs.json.

Storage Format: The file must maintain a JSON List (an array of objects).

Process:

Read the existing list from the file (if it exists).

Append the new metadata object.

Write the updated list back to the file.

Thread Safety: Ensure that concurrent chat requests do not corrupt the JSON file (use appropriate file locking or a thread-safe writing approach).

Technical Constraints:

Do NOT use RabbitMQ for this task. Keep it strictly local to the Python service. 

Ensure the logging process does not block or significantly delay the response sent back to the user.

Use clean, modular Python code.