I am building a RAG-based chatbot using a vector database (Qdrant).

Each retrieved vector contains rich metadata, including a human-readable document identifier such as document_name (e.g. “Bộ luật Lao động 2019 (Law No. 45/2019/QH14)”).

However, the current chatbot response looks like this:

“Based on the previous steps. According to Article 212 of Document #1, a branch will have its Business Location Registration Certificate revoked if it ceases operation for one year without notifying the business registration authority and tax authority. (Source: [Document #1])”

Problem:
The answer only references a placeholder like “Document #1”, instead of the actual document_name available in the vector payload.

This indicates that although semantic retrieval works, the citation logic is incorrect or incomplete.

Please analyze and verify the logic in the following components:

EmbeddingService

Check whether metadata fields such as document_name, file_name, or other identifiers are correctly embedded, stored, and retrievable from Qdrant.

Confirm that metadata is preserved and returned together with the text content during vector search.

ChatProcessor (or RAG orchestration layer)

Check whether retrieved metadata is actually passed into the prompt construction step.

Verify whether the model prompt only receives abstract labels like “Document #1” instead of the real metadata.

Identify where the mapping between retrieved chunks and their human-readable sources is lost or replaced by placeholders.

Expected behavior:
The chatbot should generate answers with explicit, human-readable citations, for example:

“According to Article 212 of Bộ luật Lao động 2019 (Law No. 45/2019/QH14) …”

Task:

Identify the exact logical flaw (EmbeddingService vs ChatProcessor vs prompt construction).

Explain why the LLM cannot access or use document_name.

Propose a concrete fix at the logic or prompt level to ensure real document names are always used instead of generic placeholders.

Focus strictly on RAG architecture, metadata flow, and prompt engineering. Do not explain legal content.