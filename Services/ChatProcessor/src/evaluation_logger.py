"""
Evaluation Logger Module

Logs RAG interaction metadata to a local JSON file for offline evaluation using Ragas.
This module provides thread-safe logging of question-answer pairs with retrieved contexts.
"""
import json
import os
import asyncio
import threading
from typing import List, Dict, Any
from datetime import datetime
from pathlib import Path
from src.logger import logger


class EvaluationLogger:
    """
    Thread-safe logger for RAG evaluation metadata.

    Logs interactions to a JSON file for later batch evaluation with Ragas framework.
    Uses threading.Lock for cross-platform thread safety.
    """

    def __init__(self, log_file_path: str = "evaluation_logs.json"):
        """
        Initialize the evaluation logger.

        Args:
            log_file_path: Path to the JSON log file (default: evaluation_logs.json in current directory)
        """
        self.log_file_path = Path(log_file_path)
        self._lock = threading.Lock()
        self._ensure_file_exists()

    def _ensure_file_exists(self):
        """Ensure the log file exists and is initialized as an empty JSON array if new."""
        if not self.log_file_path.exists():
            try:
                with open(self.log_file_path, 'w', encoding='utf-8') as f:
                    json.dump([], f)
                logger.info(f"Created new evaluation log file: {self.log_file_path}")
            except Exception as e:
                logger.error(f"Failed to create evaluation log file: {e}")

    def _read_existing_logs(self, file) -> List[Dict[str, Any]]:
        """
        Read existing logs from the file.

        Args:
            file: Open file handle

        Returns:
            List of existing log entries
        """
        try:
            file.seek(0)
            content = file.read()
            if not content or content.strip() == "":
                return []
            return json.loads(content)
        except json.JSONDecodeError:
            logger.warning("Corrupted JSON file, starting fresh")
            return []
        except Exception as e:
            logger.error(f"Error reading existing logs: {e}")
            return []

    def _write_logs(self, file, logs: List[Dict[str, Any]]):
        """
        Write logs to the file.

        Args:
            file: Open file handle
            logs: List of log entries to write
        """
        file.seek(0)
        file.truncate()
        json.dump(logs, file, ensure_ascii=False, indent=2)
        file.flush()
        os.fsync(file.fileno())

    def log_interaction(
        self,
        question: str,
        contexts: List[str],
        answer: str,
        conversation_id: int,
        user_id: int,
        tenant_id: int,
        timestamp: datetime = None
    ) -> bool:
        """
        Log a RAG interaction synchronously with thread locking for thread safety.

        Args:
            question: The original user query
            contexts: List of retrieved document chunks (Internal Policy + Global Law)
            answer: The final response generated by the LLM
            conversation_id: Unique identifier for the chat session
            user_id: Unique identifier for the user
            tenant_id: The specific ID of the enterprise (Tenant)
            timestamp: ISO 8601 formatted time (defaults to current time)

        Returns:
            bool: True if logged successfully, False otherwise
        """
        if timestamp is None:
            timestamp = datetime.utcnow()

        # Construct metadata dictionary
        metadata = {
            "question": question,
            "contexts": contexts,
            "answer": answer,
            "conversation_id": conversation_id,
            "user_id": user_id,
            "tenant_id": tenant_id,
            "timestamp": timestamp.isoformat()
        }

        try:
            # Acquire thread lock for thread safety
            with self._lock:
                # Open file and perform read-modify-write
                with open(self.log_file_path, 'r+', encoding='utf-8') as f:
                    # Read existing logs
                    existing_logs = self._read_existing_logs(f)

                    # Append new metadata
                    existing_logs.append(metadata)

                    # Write updated logs back
                    self._write_logs(f, existing_logs)

                    logger.debug(f"Logged evaluation metadata for conversation {conversation_id}")
                    return True

        except Exception as e:
            logger.error(f"Failed to log evaluation metadata: {e}", exc_info=True)
            return False

    async def log_interaction_async(
        self,
        question: str,
        contexts: List[str],
        answer: str,
        conversation_id: int,
        user_id: int,
        tenant_id: int,
        timestamp: datetime = None
    ) -> bool:
        """
        Log a RAG interaction asynchronously in a background thread to avoid blocking.

        This method runs the synchronous log_interaction in a thread pool to prevent
        blocking the event loop during file I/O operations.

        Args:
            question: The original user query
            contexts: List of retrieved document chunks
            answer: The final response generated by the LLM
            conversation_id: Unique identifier for the chat session
            user_id: Unique identifier for the user
            tenant_id: The specific ID of the enterprise
            timestamp: ISO 8601 formatted time (defaults to current time)

        Returns:
            bool: True if logged successfully, False otherwise
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            self.log_interaction,
            question,
            contexts,
            answer,
            conversation_id,
            user_id,
            tenant_id,
            timestamp
        )

    def get_logs_count(self) -> int:
        """
        Get the total number of logged interactions.

        Returns:
            int: Number of log entries
        """
        try:
            with self._lock:
                with open(self.log_file_path, 'r', encoding='utf-8') as f:
                    logs = self._read_existing_logs(f)
                    return len(logs)
        except Exception as e:
            logger.error(f"Failed to get logs count: {e}")
            return 0


# Global singleton instance
_evaluation_logger = None


def get_evaluation_logger(log_file_path: str = "evaluation_logs.json") -> EvaluationLogger:
    """
    Get or create the global evaluation logger instance.

    Args:
        log_file_path: Path to the JSON log file

    Returns:
        EvaluationLogger: The global logger instance
    """
    global _evaluation_logger
    if _evaluation_logger is None:
        _evaluation_logger = EvaluationLogger(log_file_path)
    return _evaluation_logger
